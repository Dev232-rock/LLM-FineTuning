{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcfSscfVcog2S+qv/H8Q1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev232-rock/LLM-FineTuning/blob/main/Model_FineTune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup Development Environment\n",
        "In our example, we use the [PyTorch Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) with already set up CUDA drivers and PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages."
      ],
      "metadata": {
        "id": "VZQ0mY0l74_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install Hugging Face Libraries\n",
        "!pip install \"peft==0.2.0\"\n",
        "!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"loralib\" --upgrade --quiet\n",
        "!pip install -U bitsandbytes\n",
        "# install additional dependencies needed for training\n",
        "!pip install rouge-score tensorboard py7zr"
      ],
      "metadata": {
        "id": "9E8TMQPQ76A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"knkarthick/samsum\")"
      ],
      "metadata": {
        "id": "BhMJaJ08APlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "id": "-ytSyeuTAng2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"knkarthick/samsum\")\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")\n"
      ],
      "metadata": {
        "id": "FZhRixv7A2O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To train our model, we need to convert our inputs (text) to token IDs. This is done by a ðŸ¤— Transformers Tokenizer. If you are not sure what this means, check out chapter 6 of the Hugging Face Course.**"
      ],
      "metadata": {
        "id": "Yzh68vu81guE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id=\"google/flan-t5-xxl\"\n",
        "\n",
        "# Load tokenizer of FLAN-t5-XL\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "a-XJQMK91l3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start training, we need to preprocess our data. Abstractive Summarization is a text-generation task. Our model will take a text as input and generate a summary as output. We want to understand how long our input and output will take to batch our data efficiently.\n"
      ],
      "metadata": {
        "id": "I5wjT_AQ10nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ],
      "metadata": {
        "id": "xHiq_lqC142u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We preprocess our dataset before training and save it to disk. You could run this step on your local machine or a CPU and upload it to the [HuggingFaceHub](https://huggingface.co/docs/hub/datasets-overview)**"
      ],
      "metadata": {
        "id": "pI2VXq7PN7w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for t5\n",
        "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument\n",
        "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
      ],
      "metadata": {
        "id": "3-niiKDiN9Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "YGr0IpxroOK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save datasets to disk for later easy loading\n",
        "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
        "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
      ],
      "metadata": {
        "id": "NOiYa7IWOG4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fine-Tune T5 with LoRA and bnb int-8\n",
        "\n",
        "In addition to the LoRA technique, we will use [bitsanbytes.LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)  to quantize out frozen LLM to int8. This allows us to reduce the needed memory for FLAN-T5 XXL ~4x.\n",
        "\n",
        "The first step of our training is to load the model. We are going to use [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16), which is a sharded version of [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl). The sharding will help us to not run off of memory when loading the model."
      ],
      "metadata": {
        "id": "lfQJZ5ZNlA-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "import os\n",
        "\n",
        "# huggingface hub model id\n",
        "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "\n",
        "# Define quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# Create offload directory if it doesn't exist\n",
        "offload_folder = \"./offload\"\n",
        "os.makedirs(offload_folder, exist_ok=True)\n",
        "\n",
        "# load model from the hub\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\", offload_folder=offload_folder)"
      ],
      "metadata": {
        "id": "Ln_CkkQGlBrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}